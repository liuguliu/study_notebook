{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnMtdTYoB06Dr8R8IJr7zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuguliu/study_notebook/blob/main/Megatron_LM%E7%AC%94%E8%AE%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAbuSlow4jYW"
      },
      "outputs": [],
      "source": [
        "from megatron.core import mpu, tensor_parallel\n",
        "\n",
        "mpu.initialize_model_parallel(args.tensor_model_parallel_size,\n",
        "            args.pipeline_model_parallel_size,\n",
        "            args.virtual_pipeline_model_parallel_size,\n",
        "            args.pipeline_model_parallel_split_rank)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 2D张量并行'''\n",
        "\n",
        "import colossalai\n",
        "import colossalai.nn as col_nn\n",
        "import torch\n",
        "from colossalai.utils import print_rank_0\n",
        "from colossalai.context import ParallelMode\n",
        "from colossalai.core import global_context as gpc\n",
        "from colossalai.utils import get_current_device\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=4, mode='2d'),\n",
        "))\n",
        "\n",
        "parser = colossalai.get_default_parser()\n",
        "\n",
        "colossalai.launch(config = CONFIG,\n",
        "      rank = args.rank,\n",
        "      world_size = args.world_size,\n",
        "      local_rank = rags.local_rank,\n",
        "      host = args.host,\n",
        "      port = args.port)\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, dim: int = 256):\n",
        "    super().__init__()\n",
        "    intermediate_dim = dim * 4\n",
        "    self.dense_1 = col_nn.Linear(dim, intermediate_dim)\n",
        "    print_rank_0(f'Weight of the first linear layer: {self.dense_1.weight.shape}')\n",
        "    self.activation = torch.nn.GELU()\n",
        "    self.dense_2 = col_nn.Linear(intermediate_dim, dim)\n",
        "    print_rank_0(f'Weight of the second linear layer: {self.dense_2.weight.shape}')\n",
        "    self.dropout = col_nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.dense_1(x)\n",
        "      print_rank_0(f'Output of the first linear layer: {x.shape}')\n",
        "      x = self.activation(x)\n",
        "      x = self.dense_2(x)\n",
        "      print_rank_0(f'Output of the second linear layer: {x.shape}')\n",
        "      x = self. dropout(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16,256), device = get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src = 0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_2D_COL)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_2D_ROW)]\n",
        "print_rank_0(f'Input: {x.shape}')\n",
        "\n",
        "x = m(x)\n"
      ],
      "metadata": {
        "id": "kCnBzXl6IQX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 2.5D张量并行'''\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=8, mode='2.5d', depth=2),\n",
        "))\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16,256), device=get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src=0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParalleMode.PARALLEL_2P5D_DEP)]\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_2P5D_COL)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_2P5D_ROW)]\n",
        "print_rank_0(f'Input:{x.shape}')\n",
        "\n",
        "x = m(x)"
      ],
      "metadata": {
        "id": "DDXuFceeXnyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 3D张量并行'''\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=8, model='3d'),\n",
        "))\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16, 256), device = get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src = 0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_WEIGHT)]\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_INPUT)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_OUTPUT)]\n",
        "print_rank_0(f'Input: {x.shape}')\n",
        "\n",
        "x = m(x)"
      ],
      "metadata": {
        "id": "TxOmmEshfYcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''pytorch张量并行'''\n",
        "\n",
        "from torch.distributed._tensor import DeviceMesh\n",
        "from torch.distributed.tensor.parallel import PairwiseParallel, parallelize_module\n",
        "\n",
        "# 通过设备网络根据给定的 world_size 创建分片计划\n",
        "device_mesh = DeviceMesh(\"cuda\", torch.arange(0, args.world_size))\n",
        "\n",
        "# 创建模型并移动到GPU\n",
        "model = ToyModel().cuda(rank)\n",
        "\n",
        "# 为并行化模块创建优化器\n",
        "LR = 0.25\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
        "\n",
        "# 根据给定的并行风格并行化模块（这里指定为PairwiseParallel），\n",
        "# colwise 和 rowwise 样式串联为固定对，就像【Megatron-LM】(https://arxiv.org/abs/1909.08053)所做的那样。\n",
        "model = parallellize_module(model, device_mesh, PairwiseParallel())\n",
        "\n",
        "# 对分片模块执行多次前向/后向传播和优化器对参数进行更新\n",
        "for i in range(args.iter_nums):\n",
        "  # 对于 TP, 所有 TP rank的输入需要相同。\n",
        "  # 设置随机种子是为了模块数据加载器的行为。\n",
        "  if rank==0:\n",
        "    print(f\"-----------{i}-----------------\")\n",
        "  torch.manual_seed(i)\n",
        "  inp = torch.rand(20,10).cuda(rank)\n",
        "  if rank==0:\n",
        "    print(f\"rank: {rank}, input shape: {inp.shape}\")\n",
        "  output = model(inp)\n",
        "  if rank==0:\n",
        "    print(f\"rank: {rank}, input shape: {output.shape}\")\n",
        "  output.sum().backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "eK8jbyoTienQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}