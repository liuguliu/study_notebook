{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuguliu/study_notebook/blob/main/Megatron_LM%E7%AC%94%E8%AE%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_ARGS=\" \\\n",
        "  --tensor-model-parallel-size 4 \\\n",
        "  --pipeline-mode-parallel-size 2 \\\n",
        "  --sequence-parallel \\\n",
        "  --num-layers 32 \\\n",
        "  --hidden-size 2560 \\\n",
        "  --num-attention-heads 32 \\\n",
        "  --seq-length 1024 \\\n",
        "  --max-position-embeddings 1024 \\\n",
        "  --micro-batch-size 4 \\\n",
        "  --global-batch-size 1024 \\\n",
        "  --lr 0.8e-4 \\\n",
        "  --train-iters 500000 \\\n",
        "  --lr-decay-style cosine \\\n",
        "  --min-lr 0.8e-5 \\\n",
        "  --weight-decay 1e-1 \\\n",
        "  --lr-warmup-fraction .01 \\\n",
        "  --clip-grad 1.0 \\\n",
        "  --fp16 \\\n",
        "  --use-rotary-position-embeddings \\\n",
        "  --no-gradient-accumulation-fusion \\\n",
        "  --recompute-activations \\\n",
        "\"\n",
        "print(GPT_ARGS)\n",
        "\n",
        "DATA_ARGS=\" \\\n",
        "  --data-path $DATA_PATH \\\n",
        "  --vocab-file $VOCAB_FILE \\\n",
        "  --merge-file $MERGE_FILE \\\n",
        "  --data-impl mmap \\\n",
        "  --split 949501 \\\n",
        "\"\n",
        "print(DATA_ARGS)\n",
        "\n",
        "OUTPUT_ARGS=\" \\\n",
        "  --log-interval 1 \\\n",
        "  --tensorboard-dir $TENSORBOARD_PATH \\\n",
        "  --save-interval 10000 \\\n",
        "  --eval-interval 1000 \\\n",
        "  --eval-iters 10 \\\n",
        "\"\n",
        "\n",
        "torchrun $DISTRIBUTED_ARGS pretrain_gpt.py \\\n",
        "  $GPT_ARGS \\\n",
        "  $DATA_ARGS \\\n",
        "  $OUTPUT_ARGS \\\n",
        "  --distributed-backend nccl \\\n",
        "  --save $CHECKPOINT_PATH \\\n",
        "  --load $CHECKPOINT_PATH\n",
        "\n"
      ],
      "metadata": {
        "id": "AOKRH6mvBxhT",
        "outputId": "4ce98674-c141-48c0-c942-4806fb68e992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   --tensor-model-parallel-size 4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAbuSlow4jYW"
      },
      "outputs": [],
      "source": [
        "from megatron.core import mpu, tensor_parallel\n",
        "\n",
        "mpu.initialize_model_parallel(args.tensor_model_parallel_size,\n",
        "            args.pipeline_model_parallel_size,\n",
        "            args.virtual_pipeline_model_parallel_size,\n",
        "            args.pipeline_model_parallel_split_rank)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 2D张量并行'''\n",
        "\n",
        "import colossalai\n",
        "import colossalai.nn as col_nn\n",
        "import torch\n",
        "from colossalai.utils import print_rank_0\n",
        "from colossalai.context import ParallelMode\n",
        "from colossalai.core import global_context as gpc\n",
        "from colossalai.utils import get_current_device\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=4, mode='2d'),\n",
        "))\n",
        "\n",
        "parser = colossalai.get_default_parser()\n",
        "\n",
        "colossalai.launch(config = CONFIG,\n",
        "      rank = args.rank,\n",
        "      world_size = args.world_size,\n",
        "      local_rank = rags.local_rank,\n",
        "      host = args.host,\n",
        "      port = args.port)\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, dim: int = 256):\n",
        "    super().__init__()\n",
        "    intermediate_dim = dim * 4\n",
        "    self.dense_1 = col_nn.Linear(dim, intermediate_dim)\n",
        "    print_rank_0(f'Weight of the first linear layer: {self.dense_1.weight.shape}')\n",
        "    self.activation = torch.nn.GELU()\n",
        "    self.dense_2 = col_nn.Linear(intermediate_dim, dim)\n",
        "    print_rank_0(f'Weight of the second linear layer: {self.dense_2.weight.shape}')\n",
        "    self.dropout = col_nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.dense_1(x)\n",
        "      print_rank_0(f'Output of the first linear layer: {x.shape}')\n",
        "      x = self.activation(x)\n",
        "      x = self.dense_2(x)\n",
        "      print_rank_0(f'Output of the second linear layer: {x.shape}')\n",
        "      x = self. dropout(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16,256), device = get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src = 0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_2D_COL)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_2D_ROW)]\n",
        "print_rank_0(f'Input: {x.shape}')\n",
        "\n",
        "x = m(x)\n"
      ],
      "metadata": {
        "id": "kCnBzXl6IQX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 2.5D张量并行'''\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=8, mode='2.5d', depth=2),\n",
        "))\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16,256), device=get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src=0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParalleMode.PARALLEL_2P5D_DEP)]\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_2P5D_COL)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_2P5D_ROW)]\n",
        "print_rank_0(f'Input:{x.shape}')\n",
        "\n",
        "x = m(x)"
      ],
      "metadata": {
        "id": "DDXuFceeXnyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''colossalai 3D张量并行'''\n",
        "\n",
        "# 并行设置\n",
        "CONFIG = dict(parallel = dict(\n",
        "    data = 1,\n",
        "    pipeline = 1,\n",
        "    tensor = dict(size=8, model='3d'),\n",
        "))\n",
        "\n",
        "# 创建模型\n",
        "m = MLP()\n",
        "\n",
        "# 随机输入一些数据来运行这个模型\n",
        "x = torch.randn((16, 256), device = get_current_device())\n",
        "\n",
        "# partition input\n",
        "torch.distributed.broadcast(x, src = 0)\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_WEIGHT)]\n",
        "x = torch.chunk(x, 2, dim=0)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_INPUT)]\n",
        "x = torch.chunk(x, 2, dim=-1)[gpc.get_local_rank(ParallelMode.PARALLEL_3D_OUTPUT)]\n",
        "print_rank_0(f'Input: {x.shape}')\n",
        "\n",
        "x = m(x)"
      ],
      "metadata": {
        "id": "TxOmmEshfYcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''pytorch张量并行'''\n",
        "\n",
        "from torch.distributed._tensor import DeviceMesh\n",
        "from torch.distributed.tensor.parallel import PairwiseParallel, parallelize_module\n",
        "\n",
        "# 通过设备网络根据给定的 world_size 创建分片计划\n",
        "device_mesh = DeviceMesh(\"cuda\", torch.arange(0, args.world_size))\n",
        "\n",
        "# 创建模型并移动到GPU\n",
        "model = ToyModel().cuda(rank)\n",
        "\n",
        "# 为并行化模块创建优化器\n",
        "LR = 0.25\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
        "\n",
        "# 根据给定的并行风格并行化模块（这里指定为PairwiseParallel），\n",
        "# colwise 和 rowwise 样式串联为固定对，就像【Megatron-LM】(https://arxiv.org/abs/1909.08053)所做的那样。\n",
        "model = parallellize_module(model, device_mesh, PairwiseParallel())\n",
        "\n",
        "# 对分片模块执行多次前向/后向传播和优化器对参数进行更新\n",
        "for i in range(args.iter_nums):\n",
        "  # 对于 TP, 所有 TP rank的输入需要相同。\n",
        "  # 设置随机种子是为了模块数据加载器的行为。\n",
        "  if rank==0:\n",
        "    print(f\"-----------{i}-----------------\")\n",
        "  torch.manual_seed(i)\n",
        "  inp = torch.rand(20,10).cuda(rank)\n",
        "  if rank==0:\n",
        "    print(f\"rank: {rank}, input shape: {inp.shape}\")\n",
        "  output = model(inp)\n",
        "  if rank==0:\n",
        "    print(f\"rank: {rank}, input shape: {output.shape}\")\n",
        "  output.sum().backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "eK8jbyoTienQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDP(Distributed Data Parallel)\n"
      ],
      "metadata": {
        "id": "go06XFx-sjKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import t dist\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "def example(rank, world_size):\n",
        "  # create default process group\n",
        "  dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "  # create local model\n",
        "  model = nn.Linear(10,10).to(rank)\n",
        "  # construct DDP model\n",
        "  ddp_model = DDP(model, device_ids=[rank])\n",
        "  # define loss function and optimizer\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
        "\n",
        "  # forward pass\n",
        "  outputs = ddp_model(torch.randn(20,10).to(rank))\n",
        "  labels = torch.randn(20, 10).to(rank)\n",
        "  # backward pass\n",
        "  loss_fn(outputs, labels).backward()\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "def main():\n",
        "  world_size = 2\n",
        "  mp.spawn(example, args=(world_size,), nprocs=world_size, join=True)\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  # Environment variables which need to be\n",
        "  # set when using c10d's default \"env\"\n",
        "  # initialization mode.\n",
        "  os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "  os.environ['MASTER_PORT'] = \"29500\"\n",
        "  main()"
      ],
      "metadata": {
        "id": "5EfC4lD2sa-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FSDP (Fully Sharded Data Parallel)"
      ],
      "metadata": {
        "id": "KF8e7IY9LlMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.fsdp import(\n",
        "    FullyShardedDataParallel,\n",
        "    CPUOffload,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import(\n",
        "    default_auto_wrap_policy,\n",
        ")\n",
        "import torch.nn as nn\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(8, 4)\n",
        "    self.layer2 = nn.Linear(4, 16)\n",
        "    self.layer3 = nn.Linear(16, 4)\n",
        "\n",
        "model = DistributedDataParallel(model())\n",
        "fsdp_model = FullyShardedDataParallel(\n",
        "    model(),\n",
        "    fsdp_auto_wrap_policy = default_auto_wrap_policy,\n",
        "    cpu_offload = CPUOffload(offload_params=True),\n",
        ")"
      ],
      "metadata": {
        "id": "SZJabVKgLqeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 手动包装（Manual Wrapping）\n",
        "# https://zhuanlan.zhihu.com/p/650002268\n",
        "from torch.distributed.fsdp import(\n",
        "    FullyShardedDataParallel,\n",
        "    CPUOffload,\n",
        ")\n",
        "from torch.distributed.fsdp.wrap import(\n",
        "    enable_wrap,\n",
        "    wrap,\n",
        ")\n",
        "\n",
        "import torch.nn as nn\n",
        "from typing import Dict\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = wrap(nn.Linear(8,4))\n",
        "    self.layer2 = nn.Linear(4,16)\n",
        "    self.layer3 = wrap(nn.Linear(16,4))\n",
        "\n",
        "wrapper_kwargs = Dict(cpu_offload=CPUOffload(offload_params=True))\n",
        "with enable_wrap(wrapper_cls=FullyShardedDataParallel, **wrapper_kwargs):\n",
        "  fsdp_model = wrap(model())\n",
        "\n",
        "optim = torch.optim.Adam(fsdp_model.parameters(), lr=0.0001)\n",
        "for sample,label in next_batch():\n",
        "  out = fsdp_model(input)\n",
        "  loss = criterion(out, label)\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c-p9ZVSPMps2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}